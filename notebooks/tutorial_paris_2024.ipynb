{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51df4bad-2ecc-4465-ad97-218a4f576bc4",
   "metadata": {},
   "source": [
    "# Warm up session\n",
    "1. Load the time series 'MBA_ECG14046_data_22.out' in the ECG dataset\n",
    "2. Plot the time series\n",
    "3. Highlight the anomalies in red\n",
    "4. Calculate and display the average number of anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0894132c-61ec-4d33-8929-e8f4ae6c1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0c7f9-bda7-4058-8fd0-44589b3db967",
   "metadata": {},
   "source": [
    "## Load the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93151445-b319-4a91-b895-1c72a3c0c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths & Directories\n",
    "dataset_path = os.path.join('..', 'data', 'TSB-UAD-Public', 'ECG')\n",
    "file_name = 'MBA_ECG803_data.out'\n",
    "file_path = os.path.join(dataset_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ae192-1e86-4113-8e9a-7cc764510d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the timeseries\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "data = df[0].to_numpy()\n",
    "label = df[1].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c18b67-d15e-4e36-bcdf-3935d13a1dbc",
   "metadata": {},
   "source": [
    "## Plot the time series and its anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c012fc-4663-4f6b-bd85-441b946fa45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(data[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ec43a-4571-4b42-86c0-d9d0f7567180",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1131fdf-c441-4dd8-b0e7-7235e4d3944a",
   "metadata": {},
   "source": [
    "## Highlight the anomalies in red\n",
    "\n",
    "**Question 1:** Write a function that, given a time series and its label, generates a plot of the time series with its anomalies displayed on top of it in red.\n",
    "\n",
    "**Hint:** Use matplotlib :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58223e0-0ac9-44f5-84e3-be5936686756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the following function\n",
    "def plot_signal_with_anomalies(x, y, title=None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa21fe9-b65d-4c24-8833-2752acfa0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function here\n",
    "x = data[:10000]\n",
    "y = label[:10000]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_signal_with_anomalies(x, y, title=file_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f97aa-3102-4f08-a03c-91ad2d30351b",
   "metadata": {},
   "source": [
    "## Find the average number of anomalies for the whole dataset\n",
    "\n",
    "**Question 2:** Write a function that, given the label of a time series, returns the number of anomalies found in the label.\n",
    "\n",
    "**Example:** label = 100011111100001110001, expected output = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3346976-f651-4372-8c17-f81bc1c0369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the following function\n",
    "def compute_number_of_anomalies(y):\n",
    "    pass\n",
    "    \n",
    "compute_number_of_anomalies(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e53ff7-539b-44b1-a001-bfd74fb2a325",
   "metadata": {},
   "source": [
    "### Load the whole ECG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a712f5-2356-459e-92aa-00b8113fe916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the dataset directory with '.out' extension\n",
    "files = [f for f in os.listdir(dataset_path) if f.endswith('.out')]\n",
    "files_list = []\n",
    "\n",
    "# Load them\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(dataset_path, file_name)\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    files_list.append({\n",
    "        \"dataset\": \"ECG\",\n",
    "        \"file_name\": file_name,\n",
    "        \"data\": df[0].to_numpy(),\n",
    "        \"label\": df[1].to_numpy(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a287a-8afe-4520-a568-05c4a25eb966",
   "metadata": {},
   "source": [
    "### Calculate the average number of anomalies in the ECG dataset\n",
    "\n",
    "**Question 3:** Calculate the average number of anomalies in the ECG dataset and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86141ab2-e1b6-496a-b9a8-4201bb563a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the average number of anomalies in all files\n",
    "avg_n_anomalies = 0\n",
    "\n",
    "start = time.time()\n",
    "for file in files_list:\n",
    "    pass\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Average number of anomalies per file is {n_anomalies:.2f} computed in {(end - start):.2f} secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1efda4-24a2-4726-8f0b-a80f852a1702",
   "metadata": {},
   "source": [
    "# Detecting anomalies\n",
    "1. Run 5 anomaly detectors on the same time series, and collect their scores in the list of dictionaries named *scores*\n",
    "2. Plot the anomaly scores along with the time series and its anomalies in subplots\n",
    "3. Calculate the accuracy of the anomaly detection using the following metrics:\n",
    "   - Threshold-based accuracy\n",
    "   - Area Under the Precision-Recall Curve (AUC-PR)\n",
    "   - Volume Under the Precision-Recall Curve (VUR-PR)\n",
    "4. Discuss on what do you think is the most effective anomaly detection method based on the observed results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa691524-b2d9-4b99-85d8-1e5bbd2b7adb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4003ac-4964-4975-be30-2d6114887f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tsb_kit.models.feature import Window\n",
    "from tsb_kit.utils.slidingWindows import find_length\n",
    "\n",
    "from tsb_kit.models.iforest import IForest\n",
    "from tsb_kit.models.matrix_profile import MatrixProfile\n",
    "from tsb_kit.models.pca import PCA\n",
    "from tsb_kit.models.poly import POLY\n",
    "from tsb_kit.models.ocsvm import OCSVM\n",
    "from tsb_kit.models.damp import DAMP\n",
    "from tsb_kit.models.sand import SAND\n",
    "\n",
    "# Ignoring warning about deprecated function calls, use 'default' to see warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371ed6e-a81c-4461-94ce-58d8cdf08256",
   "metadata": {},
   "source": [
    "## Run 5 detectors and collect their scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78144d06-886b-4a63-a190-1da59351004e",
   "metadata": {},
   "source": [
    "### Prepare the data for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca197f-20f9-4f7f-b056-c29a95e1022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first 10000 points.\n",
    "x = data[:10000]\n",
    "y = label[:10000]\n",
    "\n",
    "# Determine the appropriate length for the sliding window\n",
    "slidingWindow = find_length(x)\n",
    "\n",
    "# Convert the data into sliding windows\n",
    "X_data = Window(window=slidingWindow).convert(x).to_numpy()\n",
    "\n",
    "print(\"Estimated Subsequence length: \", slidingWindow)\n",
    "print(\"Time series length: \", len(x))\n",
    "print(\"Number of abnormal points: \", list(y).count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b2f26-90cc-434c-98cf-749db4642bad",
   "metadata": {},
   "source": [
    "### Compute the scores and save the results in a list of dictionaries\n",
    "\n",
    "**Question 4:** Use the 5 classifiers, mentioned in the following cells, to compute the anomaly score of x. The first classifier is already implemented. Repeat for the rest\n",
    "\n",
    "**Hint:** You can visit the repo of *tsb_kit* to check out the implementation of the detectors. Make sure that the produced anomaly scores, and the input time series, have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52996a1-f77b-4f3b-95ab-fe50e8c92b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "model_name = 'DAMP'\n",
    "\n",
    "clf = DAMP(m=slidingWindow, sp_index=slidingWindow + 1)\n",
    "clf.fit(x)\n",
    "\n",
    "score = clf.decision_scores_\n",
    "score = MinMaxScaler(feature_range=(0, 1)).fit_transform(score.reshape(-1, 1)).ravel()\n",
    "score = np.array([score[0]] * math.ceil((slidingWindow - 1) / 2) + list(score) + [score[-1]] * ((slidingWindow - 1) // 2))\n",
    "\n",
    "scores.append({\n",
    "    \"model_name\": model_name,\n",
    "    \"score\": score,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04b41d-c824-424d-b167-802b73cc2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='IForest'\n",
    "\n",
    "# TODO: Repeat the same steps for IForest\n",
    "pass\n",
    "\n",
    "scores.append({\n",
    "    \"model_name\": model_name,\n",
    "    \"score\": score,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d5636-e208-43c4-9af5-d2d7ed019296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This detector is slow only the first time we run it\n",
    "model_name = 'SAND'\n",
    "\n",
    "# TODO: Repeat the same steps for SAND\n",
    "pass\n",
    "\n",
    "scores.append({\n",
    "    \"model_name\": model_name,\n",
    "    \"score\": score,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad397267-1d58-45c2-9c2c-4d66c48ab268",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MatrixProfile'\n",
    "\n",
    "# TODO: Repeat the same steps for MatrixProfile\n",
    "pass\n",
    "\n",
    "scores.append({\n",
    "    \"model_name\": model_name,\n",
    "    \"score\": score,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1dbc3-c6f6-4ba9-b1fc-07c9e678aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='PCA'\n",
    "\n",
    "# TODO: Repeat the same steps for PCA\n",
    "pass\n",
    "\n",
    "scores.append({\n",
    "    \"model_name\": model_name,\n",
    "    \"score\": score,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3e591-4a60-4275-8619-aa335c018648",
   "metadata": {},
   "source": [
    "## Plot the anomaly scores\n",
    "\n",
    "**Question 5:** Which method do you think is the best, and which is the worst? Why? You can write your answer in a cell below the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5446d7e-e902-4f2b-b212-0653b65723c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = MinMaxScaler(feature_range=(0, 1)).fit_transform(x.reshape(-1, 1)).ravel()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(len(scores) + 1, 1, 1)\n",
    "plot_signal_with_anomalies(x, y, title=file_path)\n",
    "\n",
    "for i, elem in enumerate(scores):\n",
    "    plt.subplot(len(scores) + 1, 1, i + 2)\n",
    "    plt.title(elem['model_name'])\n",
    "    plt.plot(elem['score'], color='#D30AFA')\n",
    "    plt.plot(x_scaled, alpha=0.3)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2693e-e9f2-4e4d-8a85-a0e6e0fbafad",
   "metadata": {},
   "source": [
    "**Write your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cdaf31-8d9f-4a3a-88e7-9120f5fc7463",
   "metadata": {},
   "source": [
    "## Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce042b-0fc0-4d1b-b39f-6d960727ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_curve, auc\n",
    "from tsb_kit.vus.metrics import get_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e7de3-1c7c-4d07-961e-fd9f88832ceb",
   "metadata": {},
   "source": [
    "### Find the threshold for each score and save it in the 'scores' list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741298c-fa0c-43c0-9c1c-05910606025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, elem in enumerate(scores):\n",
    "    elem['threshold'] = elem['score'].mean() + (3 * elem['score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fd791-199b-48c2-9891-cdfd929f6c9e",
   "metadata": {},
   "source": [
    "### Compute the F1-score for each score and save it\n",
    "\n",
    "**Question 6:** Use the anomaly threshold determined previously to compute the predicted label of 1s and 0s. Then, use it to compute the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319cef19-2894-4981-8974-4f18b58b255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, elem in enumerate(scores):\n",
    "    # elem['predicted_labels'] = ?\n",
    "    # elem['f1_score'] = ?\n",
    "for elem in scores: print(f'{elem[\"model_name\"]}: {elem[\"f1_score\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00c601-6700-4bcd-a943-53280e7426f3",
   "metadata": {},
   "source": [
    "### Visualize the 5 scores along with their thresholds and F1-scores. What do you notice?\n",
    "\n",
    "**Question 7:** Why IForest, SAND and PCA have different F1-scores, although they seem to have the same accuracy? Write your answer in a cell below the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b534816-ce53-4bba-a800-62c9eb3a2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i, elem in enumerate(scores):\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    plt.hlines(y=elem['threshold'], xmin=0, xmax=len(elem['score']), color='red', alpha=0.5)\n",
    "    plt.fill_between(x=np.arange(0, len(elem['score'])), y1=elem['threshold'], y2=1, color='red', alpha=0.4)\n",
    "    plt.plot(elem['score'])\n",
    "    plt.title(f\"{elem['model_name']} F1-score: {elem['f1_score']:.2f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7ab0b-49e5-4471-a6c3-1d548aa617c3",
   "metadata": {},
   "source": [
    "**Write your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772cc42-afd6-4150-9c33-33982e298ca3",
   "metadata": {},
   "source": [
    "### Compute Precision and Recall and save them\n",
    "\n",
    "**Question 8:** Fill in the code that's missing. Compute precision and recall for multiple different thresholds and save them in the list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6234196-6aeb-4328-8591-f0497e08931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in scores:\n",
    "    # elem['precision'], elem['recall'], elem['threshold_list'] = ?\n",
    "    elem['threshold_list'] = [elem['threshold_list'][0]] + list(elem['threshold_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fdfc4f-8f45-44a5-81a7-d928d1193705",
   "metadata": {},
   "source": [
    "### Plot Precision, Recall and the Precision-Recall curve\n",
    "\n",
    "**Question 9:** Which method do you now think is the best, and why? Write your answer in a cell below the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854dde9-cb2d-44b4-941b-d24f205bf49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Precision-Thresholds\n",
    "plt.subplot(1, 2, 1)\n",
    "for elem in scores:\n",
    "    plt.plot(elem['threshold_list'], elem['precision'], label=elem['model_name'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Recall-Thresholds\n",
    "plt.subplot(1, 2, 2)\n",
    "for elem in scores:\n",
    "    plt.plot(elem['threshold_list'], elem['recall'], label=elem['model_name'])\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(10, 12))\n",
    "\n",
    "for i, elem in enumerate(scores):\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    plt.plot(elem['recall'], elem['precision'], label=elem['model_name'], color=f'C{i}')\n",
    "    plt.fill_between(elem['recall'], 0, elem['precision'], alpha=0.2, color=f'C{i}')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.title(f\"{elem['model_name']} Precision - Recall Curve\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19352583-ddfb-43a4-a0ad-f439e4a72be2",
   "metadata": {},
   "source": [
    "**Write your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec9309-14d6-4ddf-ace2-604e423d8cae",
   "metadata": {},
   "source": [
    "### Compute AUC-PR\n",
    "\n",
    "**Question 10:** Fill in the code that's missing. Compute the AUC-PR (Area Under the Precision-Recall Curve) score and save it in *scores*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73537067-8adf-481b-b4da-5f1dcf05ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUC-PR\n",
    "for i, elem in enumerate(scores):\n",
    "    # elem['AUC_PR'] = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f28fb1-a167-47a0-a477-59d65603b7bc",
   "metadata": {},
   "source": [
    "### Compute AUC-ROC, VUS-PR, VUS-ROC\n",
    "\n",
    "**Question 11:** Use the *get_metrics* function from the *vus* library to compute the metrics AUC-ROC, VUS-PR, VUS-ROC. Save them in *scores*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7218e-b29f-4a10-ada1-8c8feb10b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in scores:\n",
    "    # results = ?\n",
    "    for key in ['AUC_ROC', 'VUS_ROC', 'VUS_PR']:\n",
    "        elem[key] = results[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83b70e-9626-42b8-8ef3-f060d5ebd9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores[['model_name', 'f1_score', 'AUC_PR', 'AUC_ROC', 'VUS_PR', 'VUS_ROC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a569329-1137-4544-acd7-89378a3cfad8",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41052a92-0f0e-468c-b315-43e398cf7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, len(df_scores['model_name']) * 3, 3)\n",
    "bar_width = 0.5\n",
    "results_show = ['f1_score', 'AUC_PR', 'AUC_ROC', 'VUS_PR', 'VUS_ROC']\n",
    "offsets = np.arange(len(results_show)) - (len(results_show) // 2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "for key, offset in zip(results_show, offsets):\n",
    "    bars = ax.bar(x + (bar_width * offset), df_scores[key], bar_width, label=key)\n",
    "    ax.bar_label(bars, labels=[f\"{x:.2f}\" for x in df_scores[key]])\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparing all models by scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_scores['model_name'])\n",
    "ax.legend()\n",
    "\n",
    "plt.grid(color='pink', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b81e7d-0e72-4080-b9ad-2e9d2ac0b16f",
   "metadata": {},
   "source": [
    "### Measure Execution Time\n",
    "\n",
    "**Question 12:** Rerun the anomaly detectors and compute execution time. Then create the same bar plot as above but for execution time and compare the detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d62561-80cb-4941-8f5d-5d7a57af8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Bar plot for execution time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020f89a-2146-45c5-b0df-9327e9663341",
   "metadata": {},
   "source": [
    "# Analyse datasets\n",
    "\n",
    "The following part of this session is **optional**.\n",
    "1. Load three datasets\n",
    "2. Run 5 detectors on each\n",
    "3. Plot the AUC-PR and VUS-PR boxplots\n",
    "4. What's the best detector now? Is there a best detector? Does this apply to all the time series of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646de546-83e4-4ae6-b839-dd2e4be7de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tsb_kit.models.distance import Fourier\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a92d7-f3c5-4695-91b2-82ae580f41a0",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c92bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the paths of the datasets we are going to analyse and compare\n",
    "dataset_path = os.path.join('..', 'data', 'TSB-UAD-Public')\n",
    "datasets = ['SMD', 'SensorScope', 'MGAB']\n",
    "dataset_paths = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_paths.update({\n",
    "        dataset: os.path.join(dataset_path, dataset)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633edc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the dataset directory with '.out' extension\n",
    "files_list = []\n",
    "new_files_list = []\n",
    "timeseries_len = 5000\n",
    "\n",
    "for dataset in dataset_paths.keys():\n",
    "    files_list.extend([{\n",
    "    \"dataset\": dataset,\n",
    "    \"file_name\": f,\n",
    "} for i, f in enumerate(os.listdir(dataset_paths[dataset])[:20]) if f.endswith('.out') and i%2])\n",
    "\n",
    "# Load them\n",
    "for file in files_list:\n",
    "    file_path = os.path.join(dataset_paths[file['dataset']], file['file_name'])\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    tmp_data = df[0].to_numpy()\n",
    "    tmp_label = df[1].to_numpy()\n",
    "\n",
    "    # Find indices of anomalies\n",
    "    anomaly_indices = np.where(tmp_label == 1)[0]\n",
    "    \n",
    "    if len(anomaly_indices) > 0:\n",
    "        start_idx = max(0, anomaly_indices[0] - (timeseries_len // 2))\n",
    "        end_idx = min(len(tmp_data), start_idx + timeseries_len)\n",
    "        \n",
    "        # Adjust start index if end index exceeds data length\n",
    "        start_idx = max(0, end_idx - timeseries_len)\n",
    "        \n",
    "        file.update({\n",
    "            \"data\": tmp_data[start_idx:end_idx],\n",
    "            \"label\": tmp_label[start_idx:end_idx],\n",
    "        })\n",
    "        new_files_list.append(file)\n",
    "    \n",
    "df = pd.DataFrame(new_files_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9c409",
   "metadata": {},
   "source": [
    "## Run the Anomaly Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efb48b",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210b8bd-8f84-4c67-95e0-468809d7e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sliding_window'] = df['data'].apply(find_length)\n",
    "df['x_sub'] = df.progress_apply(lambda x: Window(window = x['sliding_window']).convert(x['data']).to_numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e8d4f",
   "metadata": {},
   "source": [
    "## Run 5 detectors and collect their scores\n",
    "\n",
    "**Question 13:** In each one of the function defined below, fill in the missing commands to create and fit the appropriate classifier according to the function's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798b71f-a428-47d3-8a24-b5d4982954b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_score(score, sliding_window):\n",
    "    score = MinMaxScaler(feature_range=(0, 1)).fit_transform(score.reshape(-1, 1)).ravel()\n",
    "    score = np.array([score[0]] * math.ceil((sliding_window - 1) / 2) + list(score) + [score[-1]] * ((sliding_window - 1) // 2))\n",
    "    return score\n",
    "\n",
    "def run_SAND(x, sliding_window):\n",
    "    # TODO: Create and fit the classifier\n",
    "    \n",
    "    return MinMaxScaler(feature_range=(0, 1)).fit_transform(clf.decision_scores_.reshape(-1, 1)).ravel()\n",
    "    \n",
    "def run_POLY(x, sliding_window):\n",
    "    # TODO: Create and fit the classifier\n",
    "    \n",
    "    measure = Fourier()\n",
    "    measure.detector = clf\n",
    "    measure.set_param()\n",
    "    \n",
    "    clf.decision_function(measure=measure)\n",
    "    score = clf.decision_scores_\n",
    "    score = MinMaxScaler(feature_range=(0, 1)).fit_transform(score.reshape(-1, 1)).ravel()\n",
    "\n",
    "    return score\n",
    "\n",
    "def run_PCA(x_sub, sliding_window):\n",
    "    # TODO: Create and fit the classifier\n",
    "    \n",
    "    return norm_score(clf.decision_scores_, sliding_window)\n",
    "\n",
    "def run_IForest(x_sub, sliding_window):\n",
    "    # TODO: Create and fit the classifier\n",
    "    \n",
    "    return norm_score(clf.decision_scores_, sliding_window)\n",
    "\n",
    "def run_OCSVM(X_train, X_test, sliding_window):\n",
    "    X_train_ = MinMaxScaler(feature_range=(0, 1)).fit_transform(X_train.T).T\n",
    "    X_test_ = MinMaxScaler(feature_range=(0, 1)).fit_transform(X_test.T).T\n",
    "    \n",
    "    # TODO: Create and fit the classifier\n",
    "    \n",
    "    return norm_score(clf.decision_scores_, sliding_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b4f60-a21a-4928-af40-7e41d5960c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the 5 detectors in all time series. This should take about 3 minutes\n",
    "df['SAND_score'] = df.progress_apply(lambda x: run_SAND(x['data'], x['sliding_window']), axis=1)\n",
    "df['POLY_score'] = df.progress_apply(lambda x: run_POLY(x['data'], x['sliding_window']), axis=1)\n",
    "df['PCA_score'] = df.progress_apply(lambda x: run_PCA(x['x_sub'], x['sliding_window']), axis=1)\n",
    "df['IForest_score'] = df.progress_apply(lambda x: run_IForest(x['x_sub'], x['sliding_window']), axis=1)\n",
    "df['OCSVM_score'] = df.progress_apply(lambda x: run_OCSVM(x['x_sub'][:int(0.1*len(x['x_sub']))], x['x_sub'], x['sliding_window']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae3f21",
   "metadata": {},
   "source": [
    "## Compute & Plot the AUC-PR in Boxplots\n",
    "\n",
    "**Question 14:** Implement the following function to compute and return the Area Under the Precision-Recall Curve (AUC-PR). Given the true labels (y) of a time series and the corresponding detection scores (score), complete the function to perform this calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed45cb-8119-4cc4-9025-e2f95b8246e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the following function\n",
    "def compute_auc(y, score):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f977ae47-78b2-42ac-8a31-a61bb262d183",
   "metadata": {},
   "source": [
    "**Question 15:** Fill in the commands, and use the function you just implemented, to compute AUC-PR for all time series in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f32a8-a2a3-4bbe-80fb-4089b0b57a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = [\n",
    "    'POLY',\n",
    "    'PCA',\n",
    "    'IForest',\n",
    "    'SAND',\n",
    "    'OCSVM'\n",
    "]\n",
    "\n",
    "# TODO: Fill in what's missing to compute AUC-PR for every detectors' score and every time series\n",
    "for detector in detectors:\n",
    "    df[f'{detector}_AUC_PR'] = df.progress_apply(lambda x: _________, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c5406-8c20-4861-b7bd-c8ce10b0e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    tmp_df = df[df['dataset'] == dataset].filter(like='AUC_PR')\n",
    "    tmp_df.columns = [x.replace('_AUC_PR', '') for x in tmp_df.columns]\n",
    "    labels = list(tmp_df.columns)\n",
    "    \n",
    "    axes[0, i].boxplot(tmp_df.values)\n",
    "    axes[0, i].set_xticks(ticks=np.arange(1, len(labels)+1), labels=labels)\n",
    "    axes[0, i].set_title(dataset)\n",
    "    \n",
    "    axes[0, i].set_ylabel('AUC-PR Distribution') if i == 0 else None\n",
    "    axes[0, i].grid(alpha=0.7, color='pink')\n",
    "\n",
    "    axes[1, i].bar(x=list(tmp_df.columns), height=tmp_df.mean().values)\n",
    "    axes[1, i].set_ylabel('AUC-PR Mean') if i == 0 else None\n",
    "    axes[1, i].grid(alpha=0.7, color='pink')\n",
    "\n",
    "    axes[2, i].bar(x=list(tmp_df.columns), height=tmp_df.median().values)\n",
    "    axes[2, i].set_ylabel('AUC-PR Median') if i == 0 else None\n",
    "    axes[2, i].grid(alpha=0.7, color='pink')\n",
    "    axes[2, i].set_xlabel('Detectors')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad94fa4-bd22-4f8d-8fc7-f2218b4fff70",
   "metadata": {},
   "source": [
    "# Bonus question: Can we do better ?\n",
    "1. Averaging Ensemble\n",
    "2. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a31897-4cfa-49e1-aeeb-7bb1f9c57e0f",
   "metadata": {},
   "source": [
    "## Compute Averaging Ensemble score, its AUC-PR and put it in the box plot.\n",
    "\n",
    "**Question 16:** Compute the Averaging Ensemble, that is the average of the anomaly scores that we computed before. Save it in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66476b-574c-482b-b0ac-e27dade28939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'AVG_ENS_score' column if it exists, in case we run this cell more than once\n",
    "if 'AVG_ENS_score' in df.columns:\n",
    "    df = df.drop('AVG_ENS_score', axis=1)\n",
    "\n",
    "scores_df = df.filter(like='_score')\n",
    "\n",
    "# TODO: Calculate the averaging ensemble for each time series\n",
    "# df['AVG_ENS_score'] = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7ce2e-5290-4962-9e7c-bc9c2b366fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVG_ENS_AUC_PR'] = df.progress_apply(lambda x: compute_auc(x['label'], x['AVG_ENS_score']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aeb0f4-095f-4e93-a06b-313e567ac06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(12, 12))\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    tmp_df = df[df['dataset'] == dataset].filter(like='AUC_PR')\n",
    "    tmp_df.columns = [x.replace('_AUC_PR', '') for x in tmp_df.columns]\n",
    "    labels = list(tmp_df.columns)\n",
    "    \n",
    "    axes[0, i].boxplot(tmp_df.values)\n",
    "    axes[0, i].set_xticks(ticks=np.arange(1, len(labels)+1), labels=labels)\n",
    "    axes[0, i].set_title(dataset)\n",
    "    \n",
    "    axes[0, i].set_ylabel('AUC-PR Distribution') if i == 0 else None\n",
    "\n",
    "    axes[1, i].bar(x=list(tmp_df.columns), height=tmp_df.mean().values)\n",
    "    axes[1, i].set_ylabel('AUC-PR Mean') if i == 0 else None\n",
    "\n",
    "    axes[2, i].bar(x=list(tmp_df.columns), height=tmp_df.median().values)\n",
    "    axes[2, i].set_ylabel('AUC-PR Median') if i == 0 else None\n",
    "    axes[2, i].set_xlabel('Detectors')\n",
    "\n",
    "    for k in range(3):\n",
    "        axes[k, i].grid(alpha=0.7, color='pink') \n",
    "        axes[k, i].tick_params(axis='x', labelrotation=25)\n",
    "\n",
    "# Global results\n",
    "tmp_df = df.filter(like='AUC_PR')\n",
    "tmp_df.columns = [x.replace('_AUC_PR', '') for x in tmp_df.columns]\n",
    "labels = list(tmp_df.columns)\n",
    "\n",
    "axes[0, 3].boxplot(tmp_df.values)\n",
    "axes[0, 3].set_xticks(ticks=np.arange(1, len(labels)+1), labels=labels)\n",
    "axes[0, 3].set_title(\"Global\")\n",
    "axes[0, 3].set_ylabel('AUC-PR Distribution') if i == 0 else None\n",
    "\n",
    "axes[1, 3].bar(x=list(tmp_df.columns), height=tmp_df.mean().values)\n",
    "axes[1, 3].set_ylabel('AUC-PR Mean') if i == 0 else None\n",
    "\n",
    "axes[2, 3].bar(x=list(tmp_df.columns), height=tmp_df.median().values)\n",
    "axes[2, 3].set_ylabel('AUC-PR Median') if i == 0 else None\n",
    "axes[2, 3].set_xlabel('Detectors')\n",
    "\n",
    "for k in range(3):\n",
    "    axes[k, 3].grid(alpha=0.7, color='pink') \n",
    "    axes[k, 3].tick_params(axis='x', labelrotation=25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deeb7cd-bab5-4a2a-b973-8999926945b8",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "**Question (Bonus):** Can we do better than that? If yes, why? Does it work in all cases? Visit the ADecimo webapp tool (link in the repo), find out and write your answer in the cell below. \n",
    "\n",
    "**Question (Super extra bonus):** Clone the MSAD repository and evaluate a pretrained model (or train it from scratch :) ), on the datasets we saw here. Visualize your results and paste them here along with your findings! Did you face any issues? Describe the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df2b06-e19f-4a2b-a03f-72e53c51a2ea",
   "metadata": {},
   "source": [
    "**Write your answer here:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
